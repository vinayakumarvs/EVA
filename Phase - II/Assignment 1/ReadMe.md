# Deep Learning for Text

***1.*** Perform One-hot encoding of words and characters

    a. Word level one-hot encoding (toy example): Without Using Keras
    
    b. Character level one-hot encoding (toy example) Without Using Keras
    
    c. Using Keras for word-level one-hot encoding:
    
    d. Word-level one-hot encoding with hashing trick (toy example):
    
***2.*** Using word Embedding

    a. On Keras builtid IMDB data which is Pre Tokenised
    
    b. On Non Tokenised IMDB Data and Tokensize using the GloVe Embeddings
    
    Results are:
    <img src="https://github.com/vinayakumarvs/EVA/blob/master/Assignment%2012/timing_breakdown.svg" width="100%" height="50%" >
</centre>
    
      By Not Tokenising with GloVe Embeddings
      
      Results are
      
      
      
    





























Reference: Materials/Deep Learning with Python.pdf
